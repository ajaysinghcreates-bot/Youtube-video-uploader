name: Instagram Reels to YouTube Shorts (Advanced SEO - Fixed)

on:
  schedule:
    # Run daily at 9 AM UTC
    - cron: '0 9 * * *'
  workflow_dispatch:
    inputs:
      instagram_url:
        description: 'Instagram Reel URL (optional - for manual trigger)'
        required: false
        type: string

env:
  YOUTUBE_CLIENT_ID: ${{ secrets.YOUTUBE_CLIENT_ID }}
  YOUTUBE_CLIENT_SECRET: ${{ secrets.YOUTUBE_CLIENT_SECRET }}
  YOUTUBE_REFRESH_TOKEN: ${{ secrets.YOUTUBE_REFRESH_TOKEN }}
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

jobs:
  process-and-upload:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg
          pip install --upgrade pip
          pip install --upgrade yt-dlp google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client requests beautifulsoup4 ffmpeg-python
          # ✅ Correct Whisper installation (fixes AttributeError)
          pip install --upgrade openai==1.51.0 git+https://github.com/openai/whisper.git

      - name: Download Instagram Reel
        run: |
          URL="${{ github.event.inputs.instagram_url }}"
          if [ -z "$URL" ]; then
            echo "⚠️ No Instagram URL provided. Exiting..."
            exit 1
          fi
          mkdir -p downloads
          yt-dlp -f mp4 -o "downloads/video.mp4" "$URL"

      - name: Extract Audio
        run: |
          ffmpeg -i downloads/video.mp4 -vn -acodec mp3 downloads/audio.mp3

      - name: Transcribe Audio with Whisper
        run: |
          python - <<EOF
          import whisper, json
          model = whisper.load_model("small")
          result = model.transcribe("downloads/audio.mp3")
          with open("downloads/transcript.json", "w", encoding="utf-8") as f:
              json.dump(result, f, indent=2, ensure_ascii=False)
          print("✅ Transcript saved to downloads/transcript.json")
          EOF

      - name: Generate SEO metadata
        run: |
          python - <<EOF
          import json, os, re, openai

          with open("downloads/transcript.json", "r", encoding="utf-8") as f:
              transcript = json.load(f)

          full_text = transcript.get("text", "")

          prompt = f"""
          Based on this transcript, generate optimized YouTube Shorts metadata.

          Transcript:
          {full_text}

          Return JSON only:
          {{
            "title": "catchy short title",
            "description": "200-300 word engaging description with hashtags",
            "tags": ["tag1","tag2","tag3"]
          }}
          """

          response = openai.chat.completions.create(
              model="gpt-4o-mini",
              messages=[{"role": "user", "content": prompt}],
              max_tokens=1200,
              temperature=0.7
          )

          content = response.choices[0].message.content
          match = re.search(r"\{.*\}", content, re.DOTALL)

          seo_json = {}
          if match:
              try:
                  seo_json = json.loads(match.group())
              except:
                  seo_json = {"title": "Fallback Title", "description": "Fallback Description", "tags":["shorts"]}
          else:
              seo_json = {"title": "Fallback Title", "description": "Fallback Description", "tags":["shorts"]}

          with open("downloads/seo_content.json", "w", encoding="utf-8") as f:
              json.dump(seo_json, f, indent=2)

          print("✅ SEO content generated from transcript")
          EOF

      - name: Upload Video to YouTube
        run: |
          python upload_to_youtube.py

      - name: Cleanup
        run: rm -f *.py
