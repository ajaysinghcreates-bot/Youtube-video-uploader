name: Instagram Reels to YouTube Shorts (SEO Auto)

on:
  schedule:
    # Run daily at 9 AM UTC
    - cron: '0 9 * * *'
  workflow_dispatch:
    inputs:
      instagram_url:
        description: 'Instagram Reel URL (optional - for manual trigger)'
        required: false
        type: string

env:
  YOUTUBE_CLIENT_ID: ${{ secrets.YOUTUBE_CLIENT_ID }}
  YOUTUBE_CLIENT_SECRET: ${{ secrets.YOUTUBE_CLIENT_SECRET }}
  YOUTUBE_REFRESH_TOKEN: ${{ secrets.YOUTUBE_REFRESH_TOKEN }}
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

jobs:
  process:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg
          pip install --upgrade pip
          pip install --upgrade yt-dlp google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client requests beautifulsoup4 ffmpeg-python
          # ✅ Reinstall OpenAI and HTTPX to fix proxies bug
          pip install --force-reinstall "openai==1.51.0" "httpx==0.27.2"
          pip install --upgrade git+https://github.com/openai/whisper.git

      - name: Download Instagram Reel
        run: |
          mkdir -p downloads
          if [ -n "${{ github.event.inputs.instagram_url }}" ]; then
            yt-dlp -f mp4 -o "downloads/video.mp4" "${{ github.event.inputs.instagram_url }}"
          else
            echo "No URL provided, exiting."
            exit 1
          fi

      - name: Extract Audio
        run: |
          ffmpeg -i downloads/video.mp4 -vn -acodec mp3 downloads/audio.mp3

      - name: Transcribe Audio
        run: |
          python - <<EOF
          import whisper, json
          model = whisper.load_model("base")
          result = model.transcribe("downloads/audio.mp3")
          with open("downloads/transcript.json", "w", encoding="utf-8") as f:
              json.dump(result, f, ensure_ascii=False, indent=2)
          print("✅ Transcript saved")
          EOF

      - name: Generate SEO Metadata
        run: |
          python - <<EOF
          import json, re, openai, os

          with open("downloads/transcript.json", "r", encoding="utf-8") as f:
              transcript = json.load(f)

          full_text = transcript.get("text", "")

          prompt = f"""
          Based on this transcript, generate optimized YouTube Shorts metadata.

          Transcript:
          {full_text}

          Return JSON only:
          {{
            "title": "catchy short title",
            "description": "200-300 word engaging description with hashtags",
            "tags": ["tag1","tag2","tag3"]
          }}
          """

          response = openai.chat.completions.create(
              model="gpt-4o-mini",
              messages=[{"role": "user", "content": prompt}],
              max_tokens=1200,
              temperature=0.7
          )

          content = response.choices[0].message.content
          match = re.search(r"\{.*\}", content, re.DOTALL)

          seo_json = {}
          if match:
              try:
                  seo_json = json.loads(match.group())
              except:
                  seo_json = {"title": "Fallback Title", "description": "Fallback Description", "tags":["shorts"]}
          else:
              seo_json = {"title": "Fallback Title", "description": "Fallback Description", "tags":["shorts"]}

          with open("downloads/seo_content.json", "w", encoding="utf-8") as f:
              json.dump(seo_json, f, indent=2)

          print("✅ SEO content generated from transcript")
          EOF

      - name: Show generated SEO content
        run: cat downloads/seo_content.json
